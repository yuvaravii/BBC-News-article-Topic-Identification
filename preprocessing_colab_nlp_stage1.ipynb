{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_colab_nlp_stage1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Wiest3VAuDaKOfa4Qqp9a974OhhqDxze",
      "authorship_tag": "ABX9TyNBJSXShDmmTRACF1RB2PFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvaravii/BBC-News-article-Topic-Identification/blob/main/preprocessing_colab_nlp_stage1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Description**\n",
        "\n",
        "In this project your task is to identify major themes/topics across a collection of BBC news articles. You can use clustering algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA) etc."
      ],
      "metadata": {
        "id": "VwzwDCKdCkBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mvsDjso_Bl3"
      },
      "outputs": [],
      "source": [
        "# for dataframes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "#for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "#gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora \n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "from spacy import displacy\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "import sklearn\n",
        "import keras\n",
        "\n",
        "#spacy\n",
        "import spacy \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for visualisation of data\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath='/content/drive/MyDrive/Colab Notebooks/Capstone Project/BBC article/1. bbc -raw Data set/to_csv file/data.csv'\n",
        "raw_df=pd.read_csv(filepath)\n",
        "\n",
        "#retaining the original file\n",
        "df=raw_df.copy()\n",
        "\n",
        "#changing the name of the columns\n",
        "df=df.rename(columns={'collection':'docs','Topics':'topics'})\n",
        "\n",
        "#dropping the unnecessary columns\n",
        "df=df.drop(columns={'Unnamed: 0'})"
      ],
      "metadata": {
        "id": "Vu2P2lZI_Lul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mOZlLSNQUY5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a column for knowing the length of columns\n",
        "df['doc_len']=df['docs'].apply(len)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4-FcGGl0_1Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.topics.unique()"
      ],
      "metadata": {
        "id": "7F76nS3iBx46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.groupby(by='topics',as_index=True).agg({'topics':'count'}))\n",
        "df.groupby(by='topics',as_index=True).agg({'topics':'count'}).plot(kind='barh',figsize=(12,4),fontsize=12,color='green')"
      ],
      "metadata": {
        "id": "gMB_5p-qGo8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a random content and view all along\n",
        "df['docs'][1234]"
      ],
      "metadata": {
        "id": "aRHhZbNgbv-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning Data**"
      ],
      "metadata": {
        "id": "r57RjqtMY9Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Data involves processes like:\n",
        "\n",
        "1. Converting into lower case - to avoid case sensitiveness\n",
        "2. remove html tags - to remove noise from internet downloaded content\n",
        "3. remove unaccented characters - e' to e\n",
        "\n",
        "4. Stop words removal\n",
        "5. Punctuation removal\n",
        "6. Numerical removal\n",
        "7. expanding contractions - like I'd = i would , you'r = you are..\n",
        "8. removing special characters = !@# etc\n",
        "9. standardisation - acronyms like nlp = natural language processing , however this is manual in nature..\n",
        "\n",
        "10. Normalisation - 1.reduces to unique no. of token ,,, variation in words of text is reduced ,,, reduction in redundunt information..........\n",
        "\n",
        "\n",
        "**Stemming** = stem - base words formed by just adding preposition and postposition such as Jump, jumping,jumped,jumps. however it has some inherent flaws like winning shown as win(overstemming) , data-->root word datum --> dat by machine (understemming). Sadly this is not the best normalization technique\n",
        "* faster, not accurate , easier to run\n",
        "\n",
        "\n",
        "**Lemmatization** --> a step by step procedure for reducing the words to base form  --> it takes Part of speech into consideration --> running (verb) converted to run, running (noun)- No conversion.\n",
        " * Not fast, accurate.\n",
        " Based on the applicability you can choose any of the below lemmatizers\n",
        "Wordnet Lemmatizer\n",
        "Spacy Lemmatizer\n",
        "TextBlob\n",
        "CLiPS Pattern\n",
        "Stanford CoreNLP\n",
        "Gensim Lemmatizer\n",
        "TreeTagger\n",
        "\n",
        "\n",
        "**Why such process are required?**\n",
        "1. Computer understand only binary , hence the data input for it shall be numerical, then each words has to be converted into numbers.\n",
        "Conversion into number involves vectorization.\n",
        "Vectorization - words stored in form of numbers with some direction. Like placing the Harry potter book in 'H' row of the libraries.\n",
        "they are given index. Similar to words are tokenized in form of numbers and stored in array.\n",
        "\n",
        "Still might be thinking why not converging on topic...on the way pal...So, more number of words --> more number of tokens --> more amount of space & computation --> complex computation required.\n",
        "So we shall now improve the computation power either we upgrade our system or cloud computing. There is another way, feed only necessary information.\n",
        "\n",
        "How do you think words like when,to,a,the,from etc.. are going to help in picking topic. Not only that how about punctuations, numericals, neither of them is going to help. We also should throw some light on grammer, parts of speech, tenses.\n",
        " I think shakespear must be angry for killing English Grammer though we bestow him deeply.\n",
        "\n",
        " How does this affect like words run,runs,ran,running --> Opportunity ! we can reduce this types of words also called stemming process.\n",
        "\n",
        " Lemmatization is another similar kind of technique. how about this inform,information,informed (verb,adjective,noun,adverd) this also add burden hence we over come it."
      ],
      "metadata": {
        "id": "qDtOXNwKZMQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's been often said in Machine Learning and NLP algorithms - garbage in, garbage out. We can't have state-of-the-art results without data which is as good. Let's spend this section working on cleaning and understanding our data set. NTLK is usually a popular choice for pre-processing - but is a rather outdated and we will be checking out spaCy, an industry grade text-processing package."
      ],
      "metadata": {
        "id": "MqU2nZ07j0jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Punctuations"
      ],
      "metadata": {
        "id": "L-BzGLipXWKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################ BLOCK 0 - PUNCTUATION REMOVAL AND LOWER CASE ####################\n",
        "\n",
        "df['data']=df.docs.to_list()\n",
        "\n",
        "#preprocessing of data in datalist\n",
        "\n",
        "df['data']= [re.sub('\\s*@\\s*\\s?',' ',str(datum)) for datum in df['data']]\n",
        "\n",
        "df['data']= [re.sub('\\?',' ',str(datum)) for datum in df['data']]\n",
        "\n",
        "df['data']= [re.sub('\\_',' ',str(datum)) for datum in df['data']]\n",
        "\n",
        "df['data']= [re.sub('\\s+',' ',str(datum)) for datum in df['data']]\n",
        "\n",
        "df['data']= [re.sub(\"\\'\",\" \",str(datum)) for datum in df['data']]\n",
        "\n",
        "df['data']= df['data'].str.lower()"
      ],
      "metadata": {
        "id": "YdY4BZCVj7_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['docs'][0]"
      ],
      "metadata": {
        "id": "vnW6nNw9YKTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'][0] #No upper case # No punctuations # lower case"
      ],
      "metadata": {
        "id": "envz0rYiSR_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "J4MPvh9DTw17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot remove stopwords on application such as machine translation, text summarization as there are higher chances of failing the objective."
      ],
      "metadata": {
        "id": "D9T8TnQmC2Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample=df['data'][0]"
      ],
      "metadata": {
        "id": "ugjkJu1C_-Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "id": "s6mLKCTAHjlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################### Block 1 -SENTENCE TO TOKEN OF WORDS #####################################################\n",
        "# creation of tokens of words\n",
        "\n",
        "def data_to_words(sentences_in_doc):\n",
        "  '''\n",
        "  This function helps to convert to single string to list of strings\n",
        "  '''\n",
        "  yield(gensim.utils.simple_preprocess(str(sentences_in_doc),deacc=True)) # it is in form of generator # removes numericals # % removed #Special character removal\n",
        "\n",
        "#df['doc_words']= list(data_to_words(df['data']))\n",
        "token_words=list(data_to_words(sample)) # returns lists of list tokenized words\n",
        "print(token_words)"
      ],
      "metadata": {
        "id": "6OkHJFB_H1Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stop words removal**\n",
        "\n",
        "The words which are used in sentence for making complete sense are called stop words. Used to avoid the grammatical error, to effectively communicate among human beings. However in NLP every word will be tokenized thus to convert the word to numerical.So, we reduce noise by removing stop words \n",
        "\n",
        "Also the computer does not understand the grammatical error and effective communication."
      ],
      "metadata": {
        "id": "8f78J4KyGjkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################ Block 2 - STOP WORDS REMOVAL ###############################################\n",
        "# Removal of stop words\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words.extend(['from','subject','re','edu','use','mr'])\n",
        "\n",
        "stp_wrd_rmd_list=[]\n",
        "stp_wrd_in_given_list=[] # a list filtered after removing stop words\n",
        "for lists in token_words:\n",
        "  number_of_words_in_doc=len(lists) # gives number of words in the given doc\n",
        "  for word in lists:  \n",
        "    if word not in stop_words:\n",
        "      stp_wrd_rmd_list.append(word)\n",
        "    else:\n",
        "      stp_wrd_in_given_list.append(word)\n",
        "\n",
        "number_of_words_in_doc\n",
        "print(number_of_words_in_doc,\n",
        "      len(stp_wrd_rmd_list),  # gives number of words with out stop words\n",
        "      len(stp_wrd_in_given_list)) # gives number stop words present\n",
        "\n",
        "# join the words as single sentence\n",
        "stp_wrd_rmd_list\n",
        "cleaned_sentence=' '.join(stp_wrd_rmd_list)"
      ],
      "metadata": {
        "id": "pkXFVakuLYWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentence # string format"
      ],
      "metadata": {
        "id": "KzMObK8YPAoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################# Block 3 (lemmatization)#####################################\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(cleaned_sentence) # gives token of words in form list of\n",
        "lemma_doc=\" \".join([token.lemma_ for token in doc])\n",
        "lemma_doc"
      ],
      "metadata": {
        "id": "80rd-hQaDtdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "id": "Nf9hfFP7GnsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################### Block 4 - Creation one function for cleaning #########################\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def cleaned_sentence(sentences):      # replace the sample with sentences\n",
        "\n",
        "  # Block 1 implemented\n",
        "  # creation of tokens of words\n",
        "  def data_to_words(sentences_in_doc):\n",
        "    '''\n",
        "    This function helps to convert to single string to list of strings\n",
        "    '''\n",
        "    yield(gensim.utils.simple_preprocess(str(sentences_in_doc),deacc=True)) # it is in form of generator\n",
        "\n",
        "  ## This generator removed numericals,punctuations, even %'s, removed special characters, converted them into lower case\n",
        "\n",
        "  token_words=list(data_to_words(sentences)) # returns lists of list tokenized words  # block 1 output = token_words\n",
        "\n",
        "  ################################################ Block 2 ###############################################\n",
        "  # Removal of stop words\n",
        "\n",
        "  from nltk.corpus import stopwords\n",
        "\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  stop_words.extend(['from','subject','re','edu','use','mr'])\n",
        "\n",
        "  stp_wrd_rmd_list=[]\n",
        "  stp_wrd_in_given_list=[]                                                  # a list filtered after removing stop words\n",
        "  for lists in token_words:\n",
        "    number_of_words_in_doc=len(lists)                                       # gives number of words in the given doc\n",
        "    for word in lists:  \n",
        "      if word not in stop_words:\n",
        "        stp_wrd_rmd_list.append(word)\n",
        "      else:\n",
        "        stp_wrd_in_given_list.append(word)\n",
        "\n",
        "  # join the words as single sentence\n",
        "  cleaned_sentence=' '.join(stp_wrd_rmd_list)            # block 2 output = len_of words, after removal ?, how many stopwrod =?\n",
        "\n",
        "  ############################################################# Block 3 (lemmatization)#####################################\n",
        "  nlp = spacy.load('en')\n",
        "  doc = nlp(cleaned_sentence) # gives token of words in form list of\n",
        "  lemma_doc=\" \".join([token.lemma_ for token in doc])\n",
        "  lemma_doc\n",
        "\n",
        "\n",
        "  return lemma_doc,stp_wrd_rmd_list,number_of_words_in_doc, len(stp_wrd_rmd_list),len(stp_wrd_in_given_list) \n"
      ],
      "metadata": {
        "id": "J9qHihuWPidp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentence(sample)"
      ],
      "metadata": {
        "id": "jAXBW1EkTQfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a data frame containing cleaned document.\n",
        "df[['cleaned_doc','cleaned_doc_token','num_words_in_doc','aft_rm_stpwd_wrd_num','stpwd_wrd_num_in_doc']]=[cleaned_sentence(doc) for doc in df['data']]"
      ],
      "metadata": {
        "id": "e0-fPpDLj2Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "AL_v_8vtbI9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to .csv file as it truly test the patience of a human being to run each time\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/Capstone Project/BBC article/2. Cleaned and Preprocessed data/cleaned_dataset_stg1.csv')"
      ],
      "metadata": {
        "id": "U2jqt-A6aaZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data Set from Storage"
      ],
      "metadata": {
        "id": "Sa0fWtDT8ABd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data_filepath='/content/drive/MyDrive/Colab Notebooks/Capstone Project/BBC article/2. Cleaned and Preprocessed data/cleaned_dataset_stg1.csv'\n",
        "new_df=pd.read_csv(processed_data_filepath)\n",
        "df1=new_df.copy()\n",
        "df1.head(3)"
      ],
      "metadata": {
        "id": "HNHj_-nKjiP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "details_dict=df1.groupby(by='topics').agg({'docs':'count','num_words_in_doc':'sum','aft_rm_stpwd_wrd_num':'sum','stpwd_wrd_num_in_doc':'sum'}).to_dict()\n",
        "details_df=pd.DataFrame(details_dict)\n",
        "details_df.columns\n",
        "details_df['% reduction']=details_df['stpwd_wrd_num_in_doc']/details_df['num_words_in_doc']*100\n",
        "details_df"
      ],
      "metadata": {
        "id": "f2svYKO9kyLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Figure Size\n",
        "fig, ax = plt.subplots(figsize =(12, 5))\n",
        " \n",
        "# Horizontal Bar Plot\n",
        "ax.barh(details_df.index, details_df['num_words_in_doc'],color='orange')\n",
        "ax.barh(details_df.index, details_df['aft_rm_stpwd_wrd_num'],color='black')\n",
        "\n",
        "ax.set_title('Total Number of words vs Number of words after cleaning')\n",
        "\n",
        "# Remove x, y Ticks\n",
        "ax.xaxis.set_ticks_position('none')\n",
        "ax.yaxis.set_ticks_position('none')\n",
        " \n",
        "# Add padding between axes and labels\n",
        "ax.xaxis.set_tick_params(pad = 5)\n",
        "ax.yaxis.set_tick_params(pad = 5)\n",
        " \n",
        "# Add x, y gridlines\n",
        "ax.grid(b = True, color ='black',\n",
        "        linestyle ='--', linewidth = 0.5,\n",
        "        alpha = 0.2)\n",
        " \n",
        "# Show top values\n",
        "ax.invert_yaxis()\n",
        " \n",
        "# Add annotation to bars\n",
        "for i in ax.patches:\n",
        "    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n",
        "             str(round((i.get_width()), 2)),\n",
        "             fontsize = 10, fontweight ='bold',\n",
        "             color ='grey')\n",
        "\n",
        "# Show Plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ESx3dZTzmepQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2 - Unigrams & Bigrams** "
      ],
      "metadata": {
        "id": "t4oSF24Z-9ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "def bi_tri_grams(text):\n",
        "  tokenize = nltk.word_tokenize(text)\n",
        "  bigrams = ngrams(tokenize,2)\n",
        "  trigrams=ngrams(tokenize,3)\n",
        "  return list(bigrams)"
      ],
      "metadata": {
        "id": "_U2cRusBubFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df=pd.DataFrame()\n",
        "sample_df['bi']=bi_tri_grams(df['cleaned_doc'][0])\n",
        "sample_df"
      ],
      "metadata": {
        "id": "fB21_-hr6CIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['bigrams_word']=df['cleaned_doc'].apply(bi_tri_grams)"
      ],
      "metadata": {
        "id": "W_Yv-bTD3vMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "o4vFucAb38TE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Non negative Matrix - Theme extraction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJ7qg+UE7zGVX70cDETrnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvaravii/BBC-News-article-Topic-Identification/blob/main/Non_negative_Matrix_Theme_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Description**\n",
        "\n",
        "In this project your task is to identify major themes/topics across a collection of BBC news articles. You can use clustering algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA) etc."
      ],
      "metadata": {
        "id": "HbhEhF-7w-wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jFUQU676haAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opb0md_5w2Rx"
      },
      "outputs": [],
      "source": [
        "# for dataframes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "#for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "#skelearn libraries\n",
        "from sklearn import decomposition\n",
        "\n",
        "\n",
        "#gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora \n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "from spacy import displacy\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "import sklearn\n",
        "import keras\n",
        "\n",
        "#spacy\n",
        "import spacy \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for visualisation of data\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data_filepath='/content/drive/MyDrive/Colab Notebooks/Capstone Project/BBC article/2. Cleaned and Preprocessed data/3rd_cleaned_dataset_stg.csv'\n",
        "new_df=pd.read_csv(processed_data_filepath)\n",
        "df=new_df.copy()\n",
        "df=df.drop(columns={'Unnamed: 0'})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QNqBuKyOw_ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.topics.unique()"
      ],
      "metadata": {
        "id": "YhaX-yGE0vPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# application of tf-idf vectorizer\n",
        "\n",
        "# Input corpus for the vectorizer\n",
        "corpus=df['cleaned_doc']\n",
        "\n",
        "# importing the necessary libraries for performing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# creation of vectorizing model\n",
        "vectorizer = TfidfVectorizer(stop_words=None, max_df=0.8, max_features=1000, ngram_range=(1,2)) # as sometimes the bigrams gives more meaning thus their frequency becomes significant\n",
        "vectors=vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "udtppUJ8hY0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectors)"
      ],
      "metadata": {
        "id": "E7a6lFt6lprV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "LmE6zD0tl1hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have tokenized and retrieved its significance. Lets decompose the redundant\n",
        "\n",
        "\n",
        "#**************************************** NMF -- NON NEGATIVE MATRIX FACTORIZATION **************************************\n",
        "clf = decomposition.NMF(n_components= 5, random_state=0)\n",
        "\n",
        "W1 =clf.fit_transform(vectors)   #  summation of baye's vector\n",
        "H1= clf.components_   # coeffecient matrix"
      ],
      "metadata": {
        "id": "SC5WkB-Km6GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1"
      ],
      "metadata": {
        "id": "svnPdnpxyUN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the top 15 words of 6 topics\n",
        "\n",
        "num_word=15\n",
        "\n",
        "vocab = np.array(vectorizer.get_feature_names())\n",
        "\n",
        "top_words= lambda t: [vocab[i] for i in np.argsort(t)[:-num_word-1:-1]]"
      ],
      "metadata": {
        "id": "EJVyV01LojK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words = [top_words(t) for t in H1]\n",
        "topics = [' '.join(t) for t in topic_words]"
      ],
      "metadata": {
        "id": "MtXNWHeVqqt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics\n",
        "\n",
        "#'business', 'entertainment', 'politics', 'sport', 'tech'\n",
        "\n",
        "# topic 0 ; business\n",
        "# topic 1 : sport\n",
        "# topic 2 : politics\n",
        "# topic 3 : entertainment\n",
        "# topic 4 : tech"
      ],
      "metadata": {
        "id": "4jx2uThPrL0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = [\"topics\"+str(i) for i in range(clf.n_components)]\n",
        "doc_names = [\"Docs\" + str(i) for i in range(len(df['cleaned_doc']))]\n",
        "df_doc_topic = pd.DataFrame(np.round(W1,2),columns=col_names,index=doc_names)\n",
        "significant_topic= np.argmax(df_doc_topic.values,axis=1)\n",
        "df_doc_topic['dominant_topic']=significant_topic"
      ],
      "metadata": {
        "id": "KwS258dLr5O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_doc_topic.head()"
      ],
      "metadata": {
        "id": "Z0QqAPfL4Hlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_doc_topic.rename(columns={'topics0':'business','topics1':'sport','topics2':'politics','topics3' : 'entertainment','topics4' : 'tech'})\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==0] = 'business'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==1] = 'sport'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==2] = 'politics'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==3] = 'entertainment'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==4] = 'tech'"
      ],
      "metadata": {
        "id": "GRC3fhAayq0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict1=df_doc_topic['dominant_topic'].to_dict()\n",
        "df['dominant_pred_topic']=dict1.values()"
      ],
      "metadata": {
        "id": "TKeKVweh2lnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "qxHmhp2M6ooG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_pred_df=df[df['topics']!=df['dominant_pred_topic']][['cleaned_doc', 'topics','dominant_pred_topic']]\n",
        "incorrect_pred_df"
      ],
      "metadata": {
        "id": "gJE0BZpQ5wCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_NNM=(1-len(incorrect_pred_df)/len(df))*100\n",
        "accuracy_NNM"
      ],
      "metadata": {
        "id": "RraH-EUH690S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "xnzuePpczlzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try with different set 2"
      ],
      "metadata": {
        "id": "YrJKK70p8bP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# application of tf-idf vectorizer\n",
        "\n",
        "# Input corpus for the vectorizer\n",
        "corpus=df['cleaned_doc']\n",
        "\n",
        "# importing the necessary libraries for performing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ******************************************** tweaks ,max_df=0.8 to 0.3,max_features=1000 to 2000 **********\n",
        "\n",
        "# creation of vectorizing model\n",
        "vectorizer = TfidfVectorizer(stop_words=None, max_df=0.3, max_features=2500, ngram_range=(1,2)) # as sometimes the bigrams gives more meaning thus their frequency becomes significant\n",
        "vectors=vectorizer.fit_transform(corpus)\n",
        "\n",
        "#**************************************** NMF -- NON NEGATIVE MATRIX FACTORIZATION **************************************\n",
        "clf = decomposition.NMF(n_components= 5, random_state=100)\n",
        "\n",
        "W1 =clf.fit_transform(vectors)   #  summation of baye's vector\n",
        "H1= clf.components_   # coeffecient matrix\n",
        "\n",
        "# Getting the top 15 words of 6 topics\n",
        "\n",
        "num_word=100\n",
        "vocab = np.array(vectorizer.get_feature_names())\n",
        "top_words= lambda t: [vocab[i] for i in np.argsort(t)[:-num_word-1:-1]]\n",
        "\n",
        "topic_words = [top_words(t) for t in H1]\n",
        "topics = [' '.join(t) for t in topic_words]\n",
        "\n",
        "# creation of df\n",
        "col_names = [\"topics\"+str(i) for i in range(clf.n_components)]\n",
        "doc_names = [\"Docs\" + str(i) for i in range(len(df['cleaned_doc']))]\n",
        "df_doc_topic = pd.DataFrame(np.round(W1,2),columns=col_names,index=doc_names)\n",
        "significant_topic= np.argmax(df_doc_topic.values,axis=1)\n",
        "df_doc_topic['dominant_topic']=significant_topic\n",
        "\n",
        "#**********************************************************************************************\n",
        "df_doc_topic.rename(columns={'topics0':'business','topics1':'sport','topics2':'politics','topics3' : 'entertainment','topics4' : 'tech'})\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==0] = 'business'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==1] = 'sport'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==2] = 'politics'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==3] = 'entertainment'\n",
        "df_doc_topic['dominant_topic'].loc[df_doc_topic['dominant_topic']==4] = 'tech'\n",
        "\n",
        "#**********************************************************************************************\n",
        "dict1=df_doc_topic['dominant_topic'].to_dict()\n",
        "df['dominant_pred_topic']=dict1.values()\n",
        "\n",
        "#**********************************************************************************************\n",
        "incorrect_pred_df=df[df['topics']!=df['dominant_pred_topic']][['cleaned_doc', 'topics','dominant_pred_topic']]\n",
        "\n",
        "#**********************************************************************************************\n",
        "accuracy_NNM=(1-len(incorrect_pred_df)/len(df))*100\n",
        "accuracy_NNM\n"
      ],
      "metadata": {
        "id": "NvS8_-ay0UxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names()"
      ],
      "metadata": {
        "id": "L13mzar8HedW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# application of tf-idf vectorizer\n",
        "\n",
        "# Input corpus for the vectorizer\n",
        "corpus=df['cleaned_doc']\n",
        "\n",
        "# importing the necessary libraries for performing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ******************************************** tweaks ,max_df=0.8 to 0.3,max_features=1000 to 2000 **********\n",
        "\n",
        "# creation of vectorizing model\n",
        "vectorizer = TfidfVectorizer(stop_words=None, max_df=0.3, max_features=2500, ngram_range=(1,2)) # as sometimes the bigrams gives more meaning thus their frequency becomes significant\n",
        "vectors=vectorizer.fit_transform(corpus)\n",
        "\n",
        "#**************************************** NMF -- NON NEGATIVE MATRIX FACTORIZATION **************************************\n",
        "clf = decomposition.NMF(n_components= 10, random_state=100)\n",
        "\n",
        "W1 =clf.fit_transform(vectors)   #  summation of baye's vector\n",
        "H1= clf.components_   # coeffecient matrix\n",
        "\n",
        "\n",
        "vocab = vectorizer.get_feature_names()\n",
        "for i, comp in enumerate(clf.components_):\n",
        "     vocab_comp = zip(vocab, comp)\n",
        "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "     \n",
        "     print(\"Topic \"+str(i)+\": \")\n",
        "     for t in sorted_words:\n",
        "            print(t[0],end=\" \")\n",
        "     print(\"\\n\")"
      ],
      "metadata": {
        "id": "Gs6m2QBBHIwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**\n",
        "\n",
        "\n",
        "1. We started with loading of data set. It had 5 topics with different documents in it. Imported the dataset using pandas.\n",
        "\n",
        "2. We preprocessed the data by using nltk ,sklearn libraries by removing punctuations, stop words, numerical values, removing special characters\n",
        "\n",
        "3. Performed word normalization technique such as lemmatization and stemming for identifying the root words\n",
        "\n",
        "4. We created bag of words using countvectorizers and calculated the TF-IDF for the given corpus.\n",
        "\n",
        "5. Trained model such as LDA, LSA, NMF for topic modelling and created clusters\n",
        "\n",
        "6. We were able to check the predictability of topic with topic mentioned in NMF.\n",
        "\n",
        "7. We hypertuned the LDA model with coherence and perplexity score for betterment. The resultant words are expressed in word cloud for better visualisation."
      ],
      "metadata": {
        "id": "dZItpLi-aZsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "1. Preprocessing step in our corpus has reduced the corpus almost 43% in average. Which is huge step for model imputation.\n",
        "\n",
        "2. As far the time consumption, NMF was pretty much faster relative to LDA and LSA.\n",
        "\n",
        "3. During the visualization we obeserve that pyLDAvis uninstall the pandas thus version controls becomes an issue during the usage of this library.\n",
        "\n",
        "4. Most important n_grams were given by TF-IDF. Statistical model were performed like LDA, Decomposition model = LSA.\n",
        "\n",
        "5. The optimal measure of selection of topic were done using coherence and perplexity score."
      ],
      "metadata": {
        "id": "z7sgajtxhP_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fQi7HYCDHqQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}